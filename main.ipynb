{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于requests的单线程爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crawler.news import NewsCrawler\n",
    "crawler = NewsCrawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.save_data('data/cn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('data/cn/washed.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# averange length of news\n",
    "total = 0\n",
    "for news in tqdm(data):\n",
    "    total += len(jieba.lcut(news['content']))\n",
    "print(total/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('data/en/washed.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# averange length of news\n",
    "total = 0\n",
    "for news in tqdm(data):\n",
    "    total += len(news['content'].split())\n",
    "print(total/len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于Scrapy的并发爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy 是一个非常强大的 Python 框架，用于 Web 爬虫和数据抓取。它可以轻松地爬取网站上的数据，并将其存储在所需的格式中（如 CSV、JSON 或数据库）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先在项目根目录下创建名为`news_crawler`的Scrapy爬虫项目\n",
    "\n",
    "```bash\n",
    "scrapy startproject news_crawler\n",
    "```\n",
    "\n",
    "生成一个爬虫模板，稍后按本实验的需求修改：\n",
    "\n",
    "```bash\n",
    "scrapy genspider example quotes.toscrape.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取中文数据\n",
    "通过Scrapy框架运行爬虫：\n",
    "\n",
    "```bash\n",
    "cd news_crawler\n",
    "scrapy crawl news_spider -s CLOSESPIDER_ITEMCOUNT=10000 -s OUTPUT_DIR=\"../../data/cn\" -a language=\"cn\" -a start_keyword=\"1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取英文数据\n",
    "通过Scrapy框架运行爬虫，只是更换语言即可：\n",
    "\n",
    "```bash\n",
    "cd news_crawler\n",
    "scrapy crawl news_spider -s CLOSESPIDER_ITEMCOUNT=20000 -s OUTPUT_DIR=\"../../data/en\" -a language=\"en\" -a start_keyword=\"1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去除乱码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def wash_cn(text: str) -> str:\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    # 替换中文标点为标准形式\n",
    "    text = re.sub(r'[！!]', '！', text)  # 统一感叹号\n",
    "    text = re.sub(r'[。]', '。', text)    # 统一句号\n",
    "    text = re.sub(r'[，,]', '，', text)   # 统一逗号\n",
    "    text = re.sub(r'[\\u3000]', ' ', text) # 去掉中文全角空格\n",
    "    text = re.sub(r'[“”]', '', text)    # 去除引号\n",
    "    text = re.sub(r'[^，。？！：；“”‘’\\u4e00-\\u9fa50-9]', '', text)  # 保留中文字符、标点和阿拉伯数字\n",
    "    return text\n",
    "\n",
    "with open('data/cn/data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "for news in tqdm(data):\n",
    "    news['content'] = wash_cn(news['content'])\n",
    "    \n",
    "with open('data/cn/washed.json', 'w') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "2024-09-25 12:52:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 8349774,\n",
    " 'downloader/request_count': 23042,\n",
    " 'downloader/request_method_count/GET': 23042,\n",
    " 'downloader/response_bytes': 74200410,\n",
    " 'downloader/response_count': 23042,\n",
    " 'downloader/response_status_count/200': 23042,\n",
    " 'dupefilter/filtered': 2200,\n",
    " 'elapsed_time_seconds': 267.455448,\n",
    " 'finish_reason': 'closespider_itemcount',\n",
    " 'finish_time': datetime.datetime(2024, 9, 25, 4, 52, 45, 644444, tzinfo=datetime.timezone.utc),\n",
    " 'httpcompression/response_bytes': 217377512,\n",
    " 'httpcompression/response_count': 22651,\n",
    " 'item_scraped_count': 20025,\n",
    " 'log_count/DEBUG': 43073,\n",
    " 'log_count/INFO': 23444,\n",
    " 'memusage/max': 168902656,\n",
    " 'memusage/startup': 74584064,\n",
    " 'offsite/domains': 1,\n",
    " 'offsite/filtered': 1,\n",
    " 'request_depth_max': 47,\n",
    " 'response_received_count': 23042,\n",
    " 'scheduler/dequeued': 23042,\n",
    " 'scheduler/dequeued/memory': 23042,\n",
    " 'scheduler/enqueued': 27585,\n",
    " 'scheduler/enqueued/memory': 27585,\n",
    " 'start_time': datetime.datetime(2024, 9, 25, 4, 48, 18, 188996, tzinfo=datetime.timezone.utc)}\n",
    "2024-09-25 12:52:45 [scrapy.core.engine] INFO: Spider closed (closespider_itemcount)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def wash_en(text: str) -> str:\n",
    "    # 去除多余的空格和换行符\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    # 替换英文标点为标准形式\n",
    "    text = re.sub(r'[!]', '!', text)      # 统一感叹号\n",
    "    text = re.sub(r'[.]', '.', text)      # 统一句号\n",
    "    text = re.sub(r'[\\,]', ',', text)      # 统一逗号\n",
    "    text = re.sub(r'[;]', ';', text)      # 统一分号\n",
    "    text = re.sub(r'[\"]', '', text)    # 去除引号\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,;!?]', '', text)  # 保留英文字符、标点和阿拉伯数字\n",
    "    return text\n",
    "\n",
    "with open('data/en/data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "for news in tqdm(data):\n",
    "    news['content'] = wash_en(news['content'])\n",
    "    \n",
    "with open('data/en/washed.json', 'w') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文分词\n",
    "使用jieba分词，全部文本储存在`data/cn/tokenized.txt`中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "\n",
    "def tokenize_cn(news: str) -> list[list[str]]:\n",
    "    sentences = re.split(r'[。！？]', news)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return [jieba.lcut(s.strip()) for s in sentences]\n",
    "\n",
    "sentences: list[list[str]] = []\n",
    "\n",
    "with open('data/cn/washed.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "for news in tqdm(data):\n",
    "    sentences.extend(tokenize_cn(news['content']))\n",
    "    \n",
    "with open('data/cn/tokenized.txt', 'w') as f:\n",
    "    sentences = [' '.join(sentence) + '\\n' for sentence in sentences]\n",
    "    f.writelines(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 英文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def tokenize_en(news: str) -> list[list[str]]:\n",
    "    sentences = re.split(r'[.?!]', news)\n",
    "    sentences = [s.strip().replace(',', ',') for s in sentences if s.strip()]\n",
    "    return [s.split() for s in sentences]\n",
    "\n",
    "sentences: list[list[str]] = []\n",
    "\n",
    "with open('data/en/washed.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "for news in tqdm(data):\n",
    "    sentences.extend(tokenize_cn(news['content']))\n",
    "    \n",
    "with open('data/en/tokenized.txt', 'w') as f:\n",
    "    sentences = [' '.join(sentence) + '\\n' for sentence in sentences]\n",
    "    f.writelines(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取日期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_date(text: str) -> list[str]:\n",
    "    # 使用非捕获组 (?:) 避免捕获分隔符，如 \"年\", \"月\", \"日\"\n",
    "    patterns = [\n",
    "        r'\\d{4}(?:年|-|/)\\d{1,2}(?:月|-|/)\\d{1,2}(?:日)?',  # 完整日期：YYYY-MM-DD\n",
    "        r'\\d{4}(?:年|-|/)\\d{1,2}(?:月)?',                    # 年月：YYYY-MM\n",
    "        r'\\d{1,2}(?:月|-|/)\\d{1,2}(?:日)?'                   # 月日：MM-DD\n",
    "    ]\n",
    "    \n",
    "    dates = []\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        dates.extend(matches)\n",
    "    return dates\n",
    "\n",
    "with open('data/cn/data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    tokenized_results = tokenize_cn(data)\n",
    "    \n",
    "dates = []\n",
    "for news in tqdm(data):\n",
    "    dates.extend(extract_date(news['content']))\n",
    "\n",
    "with open('data/cn/dates.json', 'w') as f:\n",
    "    json.dump(dates, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证Chef定律"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Load the tokenized JSON files for Chinese and English words\n",
    "cn_file_path = 'data/cn/tokenized.txt'\n",
    "en_file_path = 'data/en/tokenized.txt'\n",
    "\n",
    "with open(cn_file_path, 'r', encoding='utf-8') as cn_file:\n",
    "    cn_words = cn_file.read().split()\n",
    "\n",
    "with open(en_file_path, 'r', encoding='utf-8') as en_file:\n",
    "    en_words = en_file.read().split()\n",
    "\n",
    "# Count the frequency of each word\n",
    "cn_word_freq = Counter(cn_words)\n",
    "en_word_freq = Counter(en_words)\n",
    "\n",
    "# Sort the word frequencies in descending order\n",
    "sorted_cn_freq = sorted(cn_word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_en_freq = sorted(en_word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the rank (position) and frequency for Chinese and English words\n",
    "cn_ranks = np.arange(1, len(sorted_cn_freq) + 1)\n",
    "cn_frequencies = np.array([freq for word, freq in sorted_cn_freq])\n",
    "\n",
    "en_ranks = np.arange(1, len(sorted_en_freq) + 1)\n",
    "en_frequencies = np.array([freq for word, freq in sorted_en_freq])\n",
    "\n",
    "# Convert rank and frequency to log scale\n",
    "log_cn_ranks = np.log10(cn_ranks)\n",
    "log_cn_frequencies = np.log10(cn_frequencies)\n",
    "\n",
    "log_en_ranks = np.log10(en_ranks)\n",
    "log_en_frequencies = np.log10(en_frequencies)\n",
    "\n",
    "# Fit a linear model (for log-transformed data)\n",
    "cn_fit = np.polyfit(log_cn_ranks, log_cn_frequencies, 1)\n",
    "en_fit = np.polyfit(log_en_ranks, log_en_frequencies, 1)\n",
    "\n",
    "# Generate the fitted lines\n",
    "fitted_cn_frequencies = cn_fit[0] * log_cn_ranks + cn_fit[1]\n",
    "fitted_en_frequencies = en_fit[0] * log_en_ranks + en_fit[1]\n",
    "\n",
    "# Plotting log-log data and linear fits on a regular linear scale\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Chinese words subplot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(log_cn_ranks, log_cn_frequencies, label='Original Data')\n",
    "plt.plot(log_cn_ranks, fitted_cn_frequencies, linestyle='--', label=f'Fit: slope={cn_fit[0]:.2f}')\n",
    "plt.title(\"Log-Log Plot - CN\", fontsize=14)\n",
    "plt.xlabel(\"Log Rank (base 10)\", fontsize=12)\n",
    "plt.ylabel(\"Log Frequency (base 10)\", fontsize=12)\n",
    "plt.xticks(np.arange(int(min(log_cn_ranks)), int(max(log_cn_ranks)) + 1, 1))  # Set x-axis ticks\n",
    "plt.yticks(np.arange(int(min(log_cn_frequencies)), int(max(log_cn_frequencies)) + 1, 1))  # Set y-axis ticks\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# English words subplot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(log_en_ranks, log_en_frequencies, label='Original Data')\n",
    "plt.plot(log_en_ranks, fitted_en_frequencies, linestyle='--', label=f'Fit: slope={en_fit[0]:.2f}')\n",
    "plt.title(\"Log-Log Plot - EN\", fontsize=14)\n",
    "plt.xlabel(\"Log Rank (base 10)\", fontsize=12)\n",
    "plt.ylabel(\"Log Frequency (base 10)\", fontsize=12)\n",
    "plt.xticks(np.arange(int(min(log_en_ranks)), int(max(log_en_ranks)) + 1, 1))  # Set x-axis ticks\n",
    "plt.yticks(np.arange(int(min(log_en_frequencies)), int(max(log_en_frequencies)) + 1, 1))  # Set y-axis ticks\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.cbow import CBOW\n",
    "from model.data import CBOWDataSet, CBOWDataLoader\n",
    "\n",
    "# Load the tokenized data\n",
    "cn_tokenized_file_path = 'data/cn/tokenized.txt'\n",
    "dataset = CBOWDataSet(cn_tokenized_file_path, window_size=5)\n",
    "dataset.save('data/cn/dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating coordinates: 100%|██████████| 214011/214011 [00:00<00:00, 290763.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling coordinates...\n",
      "Partitioning dataset...\n"
     ]
    }
   ],
   "source": [
    "from model.data import CBOWDataSet, CBOWDataLoader\n",
    "\n",
    "cn_tokenized_file_path = 'data/cn/dataset.json'\n",
    "dataset = CBOWDataSet(cn_tokenized_file_path)\n",
    "train_loader, valid_loader, test_loader = dataset.partition(\n",
    "    batch_size=64,\n",
    "    ratio=(0.8, 0.1, 0.1),\n",
    "    neg_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model.cbow import CBOW\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def compute_accuracy(output, targets):\n",
    "    \"\"\"\n",
    "    计算准确率。\n",
    "    :param output: 模型的输出 (batch_size, vocab_size)\n",
    "    :param targets: 真实的目标 (batch_size,)\n",
    "    :return: 准确率\n",
    "    \"\"\"\n",
    "    _, predicted = torch.max(output, dim=1)  # 获取概率最大的类别\n",
    "    correct = (predicted == targets).sum().item()  # 计算预测正确的个数\n",
    "    accuracy = correct / targets.size(0)  # 计算准确率\n",
    "    return accuracy\n",
    "\n",
    "def train(model, \n",
    "          train_loader: CBOWDataLoader, \n",
    "          valid_loader: CBOWDataLoader, \n",
    "          epochs=10, \n",
    "          lr=0.0001,\n",
    "          device='cpu',\n",
    "          log_dir='./runs/experiment'):  # log_dir 是 TensorBoard 日志目录\n",
    "\n",
    "    # 初始化 TensorBoard 记录器\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = CBOW.loss\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        losses = []\n",
    "        \n",
    "        for batch_idx, pair in enumerate(tqdm(train_loader)):\n",
    "            pair.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 前向传播\n",
    "            output = model.forward(pair.bags)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criterion(output, pair.targets, pair.negatives)\n",
    "            loss.backward()\n",
    "\n",
    "            # 更新模型参数\n",
    "            optimizer.step()\n",
    "\n",
    "            # 记录损失\n",
    "            total_loss += loss.item()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # 计算并记录准确率\n",
    "            accuracy = compute_accuracy(output, pair.targets)\n",
    "            total_accuracy += accuracy\n",
    "\n",
    "            # 每50个批次打印一次训练损失\n",
    "            if batch_idx % 50 == 0:\n",
    "                avg_loss = total_loss / (batch_idx + 1)\n",
    "                avg_accuracy = total_accuracy / (batch_idx + 1)\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Step {batch_idx}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}')\n",
    "\n",
    "                # 使用 TensorBoard 记录损失和准确率\n",
    "                writer.add_scalar('Training Loss', avg_loss, epoch * len(train_loader) + batch_idx)\n",
    "                writer.add_scalar('Training Accuracy', avg_accuracy, epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "        # 模型验证\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            total_val_accuracy = 0\n",
    "\n",
    "            for pair in valid_loader:\n",
    "                pair.to(device)\n",
    "                output = model(pair.bags)\n",
    "                val_loss = criterion(output, pair.targets, pair.negatives)\n",
    "\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                # 计算并记录验证准确率\n",
    "                val_accuracy = compute_accuracy(output, pair.targets)\n",
    "                total_val_accuracy += val_accuracy\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(valid_loader)\n",
    "            avg_val_accuracy = total_val_accuracy / len(valid_loader)\n",
    "            print(f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f}')\n",
    "\n",
    "            # 使用 TensorBoard 记录验证集的损失和准确率\n",
    "            writer.add_scalar('Validation Loss', avg_val_loss, epoch)\n",
    "            writer.add_scalar('Validation Accuracy', avg_val_accuracy, epoch)\n",
    "            \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68578 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/68578 [00:00<1:44:36, 10.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 0, Loss: 55.2366, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 70/68578 [00:00<10:13, 111.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 50, Loss: 51.6302, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 122/68578 [00:01<09:24, 121.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 100, Loss: 51.0631, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 174/68578 [00:01<09:04, 125.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 150, Loss: 50.6205, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 214/68578 [00:02<08:54, 127.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 200, Loss: 50.5440, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 268/68578 [00:02<08:53, 128.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 250, Loss: 50.2541, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 320/68578 [00:02<09:15, 122.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 300, Loss: nan, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 372/68578 [00:03<09:13, 123.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 350, Loss: nan, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 424/68578 [00:03<09:15, 122.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 400, Loss: nan, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 476/68578 [00:04<09:04, 125.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 450, Loss: nan, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 515/68578 [00:04<09:01, 125.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 500, Loss: nan, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 567/68578 [00:04<09:03, 125.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 550, Loss: nan, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 619/68578 [00:05<08:55, 126.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 600, Loss: nan, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 671/68578 [00:05<09:14, 122.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 650, Loss: nan, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 723/68578 [00:06<09:06, 124.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 700, Loss: nan, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 775/68578 [00:06<09:01, 125.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 750, Loss: nan, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 814/68578 [00:06<09:04, 124.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 800, Loss: nan, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 866/68578 [00:07<09:09, 123.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 850, Loss: nan, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 918/68578 [00:07<09:00, 125.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 900, Loss: nan, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 930/68578 [00:07<09:26, 119.35it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m CBOW(\u001b[38;5;28mlen\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mvocab), \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, epochs, lr, device, log_dir)\u001b[0m\n\u001b[1;32m     38\u001b[0m total_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     39\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 41\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpair\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/projects/wordvector/model/data.py:196\u001b[0m, in \u001b[0;36mCBOWDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msentences[coord[\u001b[38;5;241m0\u001b[39m]][coord[\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m    194\u001b[0m bag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msentences[coord[\u001b[38;5;241m0\u001b[39m]][coord[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39mpadding:coord[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m    195\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msentences[coord[\u001b[38;5;241m0\u001b[39m]][coord[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:coord[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mpadding\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 196\u001b[0m negative \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_negatives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m targets\u001b[38;5;241m.\u001b[39mappend(target)\n\u001b[1;32m    198\u001b[0m bags\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtuple\u001b[39m(bag))\n",
      "File \u001b[0;32m~/projects/wordvector/model/data.py:207\u001b[0m, in \u001b[0;36mCBOWDataLoader._get_negatives\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m    205\u001b[0m negatives \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(negatives) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneg_size:\n\u001b[0;32m--> 207\u001b[0m     negative \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m negative \u001b[38;5;241m!=\u001b[39m target:\n\u001b[1;32m    209\u001b[0m         negatives\u001b[38;5;241m.\u001b[39mappend(negative)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/random.py:332\u001b[0m, in \u001b[0;36mRandom.randint\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty range in randrange(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstop\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m istart \u001b[38;5;241m+\u001b[39m istep \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow(n)\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandint\u001b[39m(\u001b[38;5;28mself\u001b[39m, a, b):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return random integer in range [a, b], including both end points.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandrange(a, b\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CBOW(len(dataset.vocab), 100)\n",
    "train(model, train_loader, valid_loader, epochs=10, lr=0.001, device='cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
