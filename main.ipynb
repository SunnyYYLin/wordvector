{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于requests的单线程爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from news_crawler.crawler_requests import NewsCrawler\n",
    "crawler = NewsCrawler('cn', 100)\n",
    "crawler.crawl()\n",
    "crawler.save_data('data/cn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from news_crawler.crawler_requests import NewsCrawler\n",
    "crawler = NewsCrawler('en', 100)\n",
    "crawler.crawl()\n",
    "crawler.save_data('data/cn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于Scrapy的并发爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy 是一个非常强大的 Python 框架，用于 Web 爬虫和数据抓取。它可以轻松地爬取网站上的数据，并将其存储在所需的格式中（如 CSV、JSON 或数据库）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先在项目根目录下创建名为`news_crawler`的Scrapy爬虫项目\n",
    "\n",
    "```bash\n",
    "scrapy startproject news_crawler\n",
    "```\n",
    "\n",
    "生成一个爬虫模板，稍后按本实验的需求修改：\n",
    "\n",
    "```bash\n",
    "scrapy genspider example quotes.toscrape.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取中文数据\n",
    "通过Scrapy框架运行爬虫：\n",
    "\n",
    "```bash\n",
    "cd news_crawler\n",
    "scrapy crawl news_spider -s CLOSESPIDER_ITEMCOUNT=10000 -s OUTPUT_DIR=\"../../data/cn\" -a language=\"cn\" -a start_keyword=\"1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取英文数据\n",
    "通过Scrapy框架运行爬虫，只是更换语言即可：\n",
    "\n",
    "```bash\n",
    "cd news_crawler\n",
    "scrapy crawl news_spider -s CLOSESPIDER_ITEMCOUNT=20000 -s OUTPUT_DIR=\"../../data/en\" -a language=\"en\" -a start_keyword=\"1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "2024-09-25 12:52:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 8349774,\n",
    " 'downloader/request_count': 23042,\n",
    " 'downloader/request_method_count/GET': 23042,\n",
    " 'downloader/response_bytes': 74200410,\n",
    " 'downloader/response_count': 23042,\n",
    " 'downloader/response_status_count/200': 23042,\n",
    " 'dupefilter/filtered': 2200,\n",
    " 'elapsed_time_seconds': 267.455448,\n",
    " 'finish_reason': 'closespider_itemcount',\n",
    " 'finish_time': datetime.datetime(2024, 9, 25, 4, 52, 45, 644444, tzinfo=datetime.timezone.utc),\n",
    " 'httpcompression/response_bytes': 217377512,\n",
    " 'httpcompression/response_count': 22651,\n",
    " 'item_scraped_count': 20025,\n",
    " 'log_count/DEBUG': 43073,\n",
    " 'log_count/INFO': 23444,\n",
    " 'memusage/max': 168902656,\n",
    " 'memusage/startup': 74584064,\n",
    " 'offsite/domains': 1,\n",
    " 'offsite/filtered': 1,\n",
    " 'request_depth_max': 47,\n",
    " 'response_received_count': 23042,\n",
    " 'scheduler/dequeued': 23042,\n",
    " 'scheduler/dequeued/memory': 23042,\n",
    " 'scheduler/enqueued': 27585,\n",
    " 'scheduler/enqueued/memory': 27585,\n",
    " 'start_time': datetime.datetime(2024, 9, 25, 4, 48, 18, 188996, tzinfo=datetime.timezone.utc)}\n",
    "2024-09-25 12:52:45 [scrapy.core.engine] INFO: Spider closed (closespider_itemcount)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去除乱码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from utils.cleaning import clean_cn, clean_en\n",
    "\n",
    "# cn\n",
    "with open('data/cn/data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "for news in tqdm(data):\n",
    "    news['content'] = clean_cn(news['content'])\n",
    "with open('data/cn/washed.json', 'w') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "# en\n",
    "with open('data/en/data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "for news in tqdm(data):\n",
    "    news['content'] = clean_en(news['content'])\n",
    "with open('data/en/washed.json', 'w') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中文分词\n",
    "使用jieba分词，全部文本储存在`data/cn/tokenized.txt`中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from utils.tokenization import tokenize_cn, tokenize_en\n",
    "\n",
    "# cn\n",
    "sentences: list[list[str]] = []\n",
    "with open('data/cn/washed.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "for news in tqdm(data):\n",
    "    sentences.extend(tokenize_cn(news['content'], min_len=8))\n",
    "data_size = sum([len(sentence) for sentence in sentences])\n",
    "print(f\"cn data size: {data_size}\")\n",
    "with open('data/cn/tokenized.txt', 'w') as f:\n",
    "    sentences = [' '.join(sentence) + '\\n' for sentence in sentences]\n",
    "    f.writelines(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 英文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en\n",
    "sentences: list[list[str]] = []\n",
    "with open('data/en/washed.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "for news in tqdm(data):\n",
    "    sentences.extend(tokenize_en(news['content'], min_len=8))\n",
    "data_size = sum([len(sentence) for sentence in sentences])\n",
    "print(f\"en data size: {data_size}\")\n",
    "with open('data/en/tokenized.txt', 'w') as f:\n",
    "    sentences = [' '.join(sentence) + '\\n' for sentence in sentences]\n",
    "    f.writelines(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取日期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from utils.date import extract_date\n",
    "\n",
    "with open('data/cn/washed.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dates = []\n",
    "for news in tqdm(data):\n",
    "    dates.extend(extract_date(news['content']))\n",
    "\n",
    "with open('results/dates.json', 'w') as f:\n",
    "    json.dump(dates, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证Zipf定律"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Load the tokenized JSON files for Chinese and English words\n",
    "cn_file_path = 'data/cn/tokenized.txt'\n",
    "en_file_path = 'data/en/tokenized.txt'\n",
    "\n",
    "with open(cn_file_path, 'r', encoding='utf-8') as cn_file:\n",
    "    cn_words = cn_file.read().split()\n",
    "\n",
    "with open(en_file_path, 'r', encoding='utf-8') as en_file:\n",
    "    en_words = en_file.read().split()\n",
    "\n",
    "# Count the frequency of each word\n",
    "cn_word_freq = Counter(cn_words)\n",
    "en_word_freq = Counter(en_words)\n",
    "\n",
    "# Sort the word frequencies in descending order\n",
    "sorted_cn_freq = sorted(cn_word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_en_freq = sorted(en_word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the rank (position) and frequency for Chinese and English words\n",
    "cn_ranks = np.arange(1, len(sorted_cn_freq) + 1)\n",
    "cn_frequencies = np.array([freq for word, freq in sorted_cn_freq])\n",
    "\n",
    "en_ranks = np.arange(1, len(sorted_en_freq) + 1)\n",
    "en_frequencies = np.array([freq for word, freq in sorted_en_freq])\n",
    "\n",
    "# Convert rank and frequency to log scale\n",
    "log_cn_ranks = np.log10(cn_ranks)\n",
    "log_cn_frequencies = np.log10(cn_frequencies)\n",
    "\n",
    "log_en_ranks = np.log10(en_ranks)\n",
    "log_en_frequencies = np.log10(en_frequencies)\n",
    "\n",
    "# Plotting log-log data and linear fits on a regular linear scale\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Chinese words subplot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(log_cn_ranks, log_cn_frequencies, label='Original Data')\n",
    "plt.title(\"Log-Log Plot - CN\", fontsize=14)\n",
    "plt.xlabel(\"Log Rank (base 10)\", fontsize=12)\n",
    "plt.ylabel(\"Log Frequency (base 10)\", fontsize=12)\n",
    "plt.xticks(np.arange(int(min(log_cn_ranks)), int(max(log_cn_ranks)) + 1, 1))  # Set x-axis ticks\n",
    "plt.yticks(np.arange(int(min(log_cn_frequencies)), int(max(log_cn_frequencies)) + 1, 1))  # Set y-axis ticks\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# English words subplot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(log_en_ranks, log_en_frequencies, label='Original Data')\n",
    "plt.title(\"Log-Log Plot - EN\", fontsize=14)\n",
    "plt.xlabel(\"Log Rank (base 10)\", fontsize=12)\n",
    "plt.ylabel(\"Log Frequency (base 10)\", fontsize=12)\n",
    "plt.xticks(np.arange(int(min(log_en_ranks)), int(max(log_en_ranks)) + 1, 1))  # Set x-axis ticks\n",
    "plt.yticks(np.arange(int(min(log_en_frequencies)), int(max(log_en_frequencies)) + 1, 1))  # Set y-axis ticks\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析不同主题下的词频差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.wordfreq_viz import plot_wordcloud\n",
    "from utils.tokenization import tokenize_cn\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('data/cn/washed.json', 'r', encoding='utf-8') as f:\n",
    "    news = json.load(f)\n",
    "    \n",
    "with open('data/cn_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stop_words = f.read().split()\n",
    "    stop_words.extend(['年', '月', '日'])\n",
    "    \n",
    "site2news: dict[str, list[str]] = {}\n",
    "for n in tqdm(news, desc='Classifying news by site'):\n",
    "    if n['site'] not in site2news:\n",
    "        site2news[n['site']] = []\n",
    "    site2news[n['site']].append(n['content'].strip())\n",
    "\n",
    "site2news = {site: contents for site, contents in site2news.items() if 10 < len(contents) < 1000}\n",
    "site2words: dict[str, list[str]] = {} \n",
    "for site, contents in site2news.items():\n",
    "    words = []\n",
    "    for content in tqdm(contents, desc=f'Tokenizing {site}'):\n",
    "        words.extend([word for sentence in tokenize_cn(content) for word in sentence])\n",
    "    \n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    site2words[site] = words\n",
    "\n",
    "for site, words in site2words.items():\n",
    "    plot_wordcloud(words, site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练对照模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.dataset import CBOWDataSet\n",
    "from models.cbow import CBOW\n",
    "from models.word2vec import Word2Vec\n",
    "from utils.plot import plot_curves\n",
    "\n",
    "dataset = CBOWDataSet(\n",
    "\t  'data/cn/tokenized.txt', \n",
    "\t  window_size=5, \n",
    "\t  max_vocab=5_000\n",
    ")\n",
    "vocab = dataset.vocab\n",
    "train_loader, test_loader = dataset.partition(\n",
    "    batch_size=512,\n",
    "    neg_size=16,\n",
    "    ratio=0.9,\n",
    ")\n",
    "model = CBOW(len(vocab), 128)\n",
    "word2vec = Word2Vec(model, vocab)\n",
    "word2vec.train(train_loader, epochs=8, lr=1e-3, log_dir='logs/cn_baseline/')\n",
    "word2vec.save('weights/cn_baseline/')\n",
    "word2vec.test(test_loader)\n",
    "plot_curves('logs/cn_baseline/', ['Training Loss', 'Training Accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "英文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.dataset import CBOWDataSet\n",
    "from models.cbow import CBOW\n",
    "from models.word2vec import Word2Vec\n",
    "from utils.plot import plot_curves\n",
    "\n",
    "# dataset = CBOWDataSet(\n",
    "# \t  'data/en/tokenized.txt', \n",
    "# \t  window_size=5, \n",
    "# \t  max_vocab=5_000\n",
    "# )\n",
    "# vocab = dataset.vocab\n",
    "# train_loader, test_loader = dataset.partition(\n",
    "#     batch_size=512,\n",
    "#     neg_size=16,\n",
    "#     ratio=0.9,\n",
    "# )\n",
    "# model = CBOW(len(vocab), 128)\n",
    "# word2vec = Word2Vec(model, vocab)\n",
    "# word2vec.train(train_loader, epochs=8, lr=1e-3, log_dir='logs/en_baseline/')\n",
    "# word2vec.save('weights/en_baseline/')\n",
    "# word2vec.test(test_loader)\n",
    "plot_curves('logs/en_baseline/', ['Training Loss', 'Training Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from models.word2vec import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec.load('weights/cn_baseline/')\n",
    "to_find_neighbors = ['主席', '暴雨', '党', '深化', '进口'] + random.sample(word2vec.vocab.vocab, 15)\n",
    "for word in to_find_neighbors:\n",
    "    print(f\"Neighbors of {word}: {word2vec.nearest(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from models.word2vec import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec.load('weights/en_baseline/')\n",
    "to_find_neighbors = ['president', 'great', 'storm', 'deepen', 'import'] + random.sample(word2vec.vocab.vocab, 15)\n",
    "for word in to_find_neighbors:\n",
    "    print(f\"Neighbors of {word}: {word2vec.nearest(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 是否使用log-sigmoid作为loss的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cbow import CBOW\n",
    "from models.word2vec import Word2Vec\n",
    "from models.dataset import CBOWDataSet\n",
    "from utils.plot import plot_curves\n",
    "\n",
    "def test_logsigmoid(use_logsigmoid: bool):\n",
    "    model = CBOW(len(vocab), 128)\n",
    "    word2vec = Word2Vec(model, vocab)\n",
    "    word2vec.train(train_loader, epochs=8, lr=1e-3, log_dir=f'logs/cn_logsigmoid/', use_logsigmoid=use_logsigmoid)\n",
    "    word2vec.test(test_loader)\n",
    "    word2vec.save(f'weights/cn_logsigmoid/{use_logsigmoid}/')\n",
    "\n",
    "dataset = CBOWDataSet('data/cn/tokenized.txt', window_size=5, max_vocab=5_000)\n",
    "vocab = dataset.vocab\n",
    "train_loader, test_loader = dataset.partition(\n",
    "    batch_size=512,\n",
    "    neg_size=16,\n",
    "    ratio=0.9,\n",
    ")\n",
    "for use_logsigmoid in [False]:\n",
    "    test_logsigmoid(use_logsigmoid)\n",
    "    \n",
    "plot_curves('logs/cn_logsigmoid/', ['Training Accuracy', 'Training Loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 不同词表大小的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cbow import CBOW\n",
    "from models.word2vec import Word2Vec\n",
    "from models.dataset import CBOWDataSet\n",
    "from utils.plot import plot_curves\n",
    "\n",
    "def test_vocab_size(vocab_size: int):\n",
    "    dataset = CBOWDataSet('data/cn/tokenized.txt', window_size=5, max_vocab=vocab_size)\n",
    "    vocab = dataset.vocab\n",
    "    train_loader, test_loader = dataset.partition(\n",
    "        batch_size=512,\n",
    "        neg_size=16,\n",
    "        ratio=0.9,\n",
    "    )\n",
    "    model = CBOW(len(vocab), 128)\n",
    "    word2vec = Word2Vec(model, vocab)\n",
    "    word2vec.train(train_loader, epochs=8, lr=1e-3, log_dir=f'logs/cn_vocab_size/')\n",
    "    word2vec.test(test_loader)\n",
    "    word2vec.save(f'weights/cn_vocab_size/{vocab_size}/')\n",
    "    \n",
    "for vocab_size in [2_500, 5_000, 10_000, 20_000]:\n",
    "    test_vocab_size(vocab_size)\n",
    "    \n",
    "plot_curves('logs/cn_vocab_size/', ['Training Loss', 'Training Accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 不同窗口大小的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cbow import CBOW\n",
    "from models.word2vec import Word2Vec\n",
    "from models.dataset import CBOWDataSet\n",
    "from utils.plot import plot_curves\n",
    "\n",
    "def test_window_size(window_size: int):\n",
    "    dataset = CBOWDataSet('data/cn/tokenized.txt', window_size=window_size, max_vocab=5_000)\n",
    "    vocab = dataset.vocab\n",
    "    train_loader, test_loader = dataset.partition(\n",
    "        batch_size=512,\n",
    "        neg_size=16,\n",
    "        ratio=0.9,\n",
    "    )\n",
    "    model = CBOW(len(vocab), 128)\n",
    "    word2vec = Word2Vec(model, vocab)\n",
    "    word2vec.train(train_loader, epochs=8, lr=1e-3, log_dir=f'logs/cn_window_size/')\n",
    "    word2vec.test(test_loader)\n",
    "    word2vec.save(f'weights/cn_window_size/{window_size}/')\n",
    "    \n",
    "for window_size in [1, 3, 5, 7]:\n",
    "    test_window_size(window_size)\n",
    "    \n",
    "plot_curves('logs/cn_window_size/', ['Training Loss', 'Training Accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 不同负采样大小的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cbow import CBOW\n",
    "from models.word2vec import Word2Vec\n",
    "from models.dataset import CBOWDataSet\n",
    "from utils.plot import plot_curves\n",
    "\n",
    "def test_neg_size(neg_size: int):\n",
    "    model = CBOW(len(vocab), 128)\n",
    "    word2vec = Word2Vec(model, vocab)\n",
    "    word2vec.train(train_loader, epochs=8, lr=1e-3, log_dir=f'logs/cn_neg_size/')\n",
    "    word2vec.test(test_loader)\n",
    "    word2vec.save(f'weights/cn_neg_size/{neg_size}/')\n",
    "\n",
    "# dataset = CBOWDataSet('data/cn/tokenized.txt', window_size=5, max_vocab=5_000)\n",
    "vocab = dataset.vocab\n",
    "train_loader, test_loader = dataset.partition(\n",
    "    batch_size=512,\n",
    "    neg_size=16,\n",
    "    ratio=0.9,\n",
    ")\n",
    "for neg_size in [4, 8, 16, 32]:\n",
    "    test_neg_size(neg_size)\n",
    "    \n",
    "plot_curves('logs/cn_neg_size/', ['Training Loss', 'Training Accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
